// Package kafka provides a Kafka abstraction through github.com/Shopify/sarama.
//
// bps.NewPublisher supports `kafka` + `kafka+sync` schemes and the following query parameters:
//
//   client.id
//      A user-provided string sent with every request to the brokers for logging debugging, and auditing
//      purposes.
//   kafka.version
//      The version of the kafka server.
//   channel.buffer.size
//      The number of events to buffer in internal and external channels. This permits the producer to
//      continue processing some messages in the background while user code is working, greatly improving
//      throughput (default 256).
//   acks
//      The number of acks required before considering a request complete. When acks=0, the producer will
//      not wait for any acknowledgment. When acks=1 the leader will write the record to its local log but
//      will respond without awaiting full acknowledgement from all followers. When acks=all the leader will
//      wait for the full set of in-sync replicas to acknowledge the record.
//   message.max.bytes
//      The maximum permitted size of a message (default 1,000,000). Should be
//      set equal to or smaller than the broker's `message.max.bytes`.
//   compression.type
//      The compression type for all data generated by the producer. Valid values are: none, gzip, snappy,
//      lz4 (default none).
//   partitioner
//      The partitioner used to partition messages (defaults to hashing the message ID). Valid values are:
//      hash, random, roundrobin,
//   timeout
//      The maximum duration the broker will wait the receipt of the number of acks (default 10s).
//      This is only relevant when acks=all or a number > 1.
//   flush.bytes
//      he best-effort number of bytes needed to trigger a flush.
//   flush.messages
//      The best-effort number of messages needed to trigger a flush.
//   flush.frequency
//      The best-effort frequency of flushes.
//   retry.max
//      The total number of times to retry sending a message (default 3).
//   retry.backoff
//      How long to wait for the cluster to settle between retries (default 100ms).
package kafka

import (
	"context"
	"net/url"

	"github.com/Shopify/sarama"
	"github.com/bsm/bps"
)

func init() {
	bps.RegisterPublisher("kafka", func(ctx context.Context, u *url.URL) (bps.Publisher, error) {
		config := parseQuery(u.Query())
		config.Producer.Return.Errors = false
		return NewPublisher(parseAddrs(u), config)
	})

	bps.RegisterPublisher("kafka+sync", func(ctx context.Context, u *url.URL) (bps.Publisher, error) {
		config := parseQuery(u.Query())
		config.Producer.Return.Successes = true
		return NewSyncPublisher(parseAddrs(u), config)
	})
}

// --------------------------------------------------------------------

// Publisher wraps a kafka producer and implements the bps.Publisher interface.
type Publisher struct {
	producer sarama.AsyncProducer
}

// NewPublisher inits a new async publisher.
func NewPublisher(addrs []string, config *sarama.Config) (*Publisher, error) {
	producer, err := sarama.NewAsyncProducer(addrs, config)
	if err != nil {
		return nil, err
	}
	return &Publisher{producer: producer}, nil
}

// Topic implements the bps.Publisher interface.
func (p *Publisher) Topic(name string) bps.Topic {
	return &topicAsync{name: name, producer: p.producer}
}

// Close implements the bps.Publisher interface.
func (p *Publisher) Close() error {
	return p.producer.Close()
}

// Producer exposes the native producer. Use at your own risk!
func (p *Publisher) Producer() sarama.AsyncProducer {
	return p.producer
}

type topicAsync struct {
	name     string
	producer sarama.AsyncProducer
}

// Publish implements the bps.Topic interface.
func (t *topicAsync) Publish(_ context.Context, msg *bps.PubMessage) error {
	t.producer.Input() <- convertMessage(t.name, msg)
	return nil
}

// PublishBatch implements the bps.Topic interface.
func (t *topicAsync) PublishBatch(ctx context.Context, batch []*bps.PubMessage) error {
	for _, msg := range batch {
		if err := t.Publish(ctx, msg); err != nil {
			return err
		}
	}
	return nil
}

// --------------------------------------------------------------------

// SyncPublisher wraps a synchronous kafka producer and implements the bps.Publisher interface.
type SyncPublisher struct {
	producer sarama.SyncProducer
}

// NewSyncPublisher inits a new async publisher.
func NewSyncPublisher(addrs []string, config *sarama.Config) (*SyncPublisher, error) {
	producer, err := sarama.NewSyncProducer(addrs, config)
	if err != nil {
		return nil, err
	}
	return &SyncPublisher{producer: producer}, nil
}

// Topic implements the bps.Publisher interface.
func (p *SyncPublisher) Topic(name string) bps.Topic {
	return &topicSync{name: name, producer: p.producer}
}

// Close implements the bps.Publisher interface.
func (p *SyncPublisher) Close() error {
	return p.producer.Close()
}

// Producer exposes the native producer. Use at your own risk!
func (p *SyncPublisher) Producer() sarama.SyncProducer {
	return p.producer
}

type topicSync struct {
	name     string
	producer sarama.SyncProducer
}

// Publish implements the bps.Topic interface.
func (t *topicSync) Publish(_ context.Context, msg *bps.PubMessage) error {
	_, _, err := t.producer.SendMessage(convertMessage(t.name, msg))
	return err
}

// PublishBatch implements the bps.Topic interface.
func (t *topicSync) PublishBatch(ctx context.Context, batch []*bps.PubMessage) error {
	if len(batch) == 0 {
		return nil
	}

	pmsgs := make([]*sarama.ProducerMessage, len(batch))
	for i, msg := range batch {
		pmsgs[i] = convertMessage(t.name, msg)
	}
	return t.producer.SendMessages(pmsgs)
}
