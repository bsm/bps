// Package kafka provides a Kafka abstraction through github.com/Shopify/sarama.
//
// WARNING: there's no message ack-ing done by Subscriber, so no automatic resuming from last-processed message.
// Subscribing is done only from oldest-known or newest or manually-specified numeric offset.
//
// Both bps.NewPublisher (`kafka` + `kafka+sync` schemes) and bps.NewConsumer (`kafka` scheme)
// support the following query parameters:
//
//   client.id
//      A user-provided string sent with every request to the brokers for logging debugging, and auditing
//      purposes.
//   rack.id
//      A rack identifier for this client. This can be any string value which indicates where this client
//      is physically located. It corresponds with the broker config 'broker.rack'.
//   net.max.requests
//      How many outstanding requests a connection is allowed to have before sending on it blocks (default 5).
//   net.dial.timeout
//      How long to wait for the initial connection (default 30s).
//   net.read.timeout
//      How long to wait for a response (default 30s).
//   net.write.timeout
//      How long to wait for for a transmit (default 30s).
//   net.tls.enable
//      Whether or not to use TLS when connecting to the broker (defaults to false).
//   kafka.version
//      The version of the kafka server.
//   channel.buffer.size
//      The number of events to buffer in internal and external channels. This permits the producer to
//      continue processing some messages in the background while user code is working, greatly improving
//      throughput (default 256).
//
// bps.NewPublisher supports `kafka` + `kafka+sync` schemes and the following query parameters:
//
//   acks
//      The number of acks required before considering a request complete. When acks=0, the producer will
//      not wait for any acknowledgment. When acks=1 the leader will write the record to its local log but
//      will respond without awaiting full acknowledgement from all followers. When acks=all the leader will
//      wait for the full set of in-sync replicas to acknowledge the record.
//   message.max.bytes
//      The maximum permitted size of a message (default 1,000,000). Should be
//      set equal to or smaller than the broker's `message.max.bytes`.
//   compression.type
//      The compression type for all data generated by the producer. Valid values are: none, gzip, snappy,
//      lz4 (default none).
//   partitioner
//      The partitioner used to partition messages (defaults to hashing the message ID). Valid values are:
//      hash, random, roundrobin,
//   timeout
//      The maximum duration the broker will wait the receipt of the number of acks (default 10s).
//      This is only relevant when acks=all or a number > 1.
//   flush.bytes
//      he best-effort number of bytes needed to trigger a flush.
//   flush.messages
//      The best-effort number of messages needed to trigger a flush.
//   flush.frequency
//      The best-effort frequency of flushes.
//   retry.max
//      The total number of times to retry sending a message (default 3).
//   retry.backoff
//      How long to wait for the cluster to settle between retries (default 100ms).
//
// bps.NewConsumer (`kafka://` scheme) supports:
//
//   offsets.initial
//     Offset to start consuming from. Can be "oldest" (oldest available message)
//     or "newest" (only new messages - produced after subscribing)
//     or just numeric offset value.
//
package kafka

import (
	"context"
	"fmt"
	"net/url"

	"github.com/bsm/bps/internal/concurrent"

	"github.com/Shopify/sarama"
	"github.com/bsm/bps"
)

func init() {
	bps.RegisterPublisher("kafka", func(ctx context.Context, u *url.URL) (bps.Publisher, error) {
		config := parseProducerQuery(u.Query())
		config.Producer.Return.Errors = false
		return NewPublisher(parseAddrs(u), config)
	})

	bps.RegisterPublisher("kafka+sync", func(ctx context.Context, u *url.URL) (bps.Publisher, error) {
		config := parseProducerQuery(u.Query())
		config.Producer.Return.Successes = true
		return NewSyncPublisher(parseAddrs(u), config)
	})

	bps.RegisterSubscriber("kafka", func(ctx context.Context, u *url.URL) (bps.Subscriber, error) {
		config := parseSubscriberQuery(u.Query())
		config.Consumer.Return.Errors = false
		return NewSubscriber(parseAddrs(u), config)
	})
}

// --------------------------------------------------------------------

// Publisher wraps a kafka producer and implements the bps.Publisher interface.
type Publisher struct {
	producer sarama.AsyncProducer
}

// NewPublisher inits a new async publisher.
func NewPublisher(addrs []string, config *sarama.Config) (*Publisher, error) {
	producer, err := sarama.NewAsyncProducer(addrs, config)
	if err != nil {
		return nil, err
	}
	return &Publisher{producer: producer}, nil
}

// Topic implements the bps.Publisher interface.
func (p *Publisher) Topic(name string) bps.PubTopic {
	return &topicAsync{name: name, producer: p.producer}
}

// Close implements the bps.Publisher interface.
func (p *Publisher) Close() error {
	return p.producer.Close()
}

// Producer exposes the native producer. Use at your own risk!
func (p *Publisher) Producer() sarama.AsyncProducer {
	return p.producer
}

type topicAsync struct {
	name     string
	producer sarama.AsyncProducer
}

// Publish implements the bps.Topic interface.
func (t *topicAsync) Publish(_ context.Context, msg *bps.PubMessage) error {
	t.producer.Input() <- convertMessage(t.name, msg)
	return nil
}

// --------------------------------------------------------------------

// SyncPublisher wraps a synchronous kafka producer and implements the bps.Publisher interface.
type SyncPublisher struct {
	producer sarama.SyncProducer
}

// NewSyncPublisher inits a new async publisher.
func NewSyncPublisher(addrs []string, config *sarama.Config) (*SyncPublisher, error) {
	producer, err := sarama.NewSyncProducer(addrs, config)
	if err != nil {
		return nil, err
	}
	return &SyncPublisher{producer: producer}, nil
}

// Topic implements the bps.Publisher interface.
func (p *SyncPublisher) Topic(name string) bps.PubTopic {
	return &topicSync{name: name, producer: p.producer}
}

// Close implements the bps.Publisher interface.
func (p *SyncPublisher) Close() error {
	return p.producer.Close()
}

// Producer exposes the native producer. Use at your own risk!
func (p *SyncPublisher) Producer() sarama.SyncProducer {
	return p.producer
}

type topicSync struct {
	name     string
	producer sarama.SyncProducer
}

// Publish implements the bps.Topic interface.
func (t *topicSync) Publish(_ context.Context, msg *bps.PubMessage) error {
	_, _, err := t.producer.SendMessage(convertMessage(t.name, msg))
	return err
}

// --------------------------------------------------------------------

// Subscriber wraps a kafka consumer and implements the bps.Subscriber interface.
type Subscriber struct {
	consumer sarama.Consumer
}

// NewSubscriber inits a new subscriber.
// By default, it starts handling from the newest available message (published after subscribing).
func NewSubscriber(addrs []string, config *sarama.Config) (*Subscriber, error) {
	consumer, err := sarama.NewConsumer(addrs, config)
	if err != nil {
		return nil, err
	}
	return &Subscriber{consumer: consumer}, nil
}

// Topic returns named topic handle.
func (s *Subscriber) Topic(name string) bps.SubTopic {
	return &subTopic{
		consumer: s.consumer,
		name:     name,
	}
}

// Close implements the bps.Subscriber interface.
func (s *Subscriber) Close() error {
	return s.consumer.Close()
}

// ----------------------------------------------------------------------------

type subTopic struct {
	consumer sarama.Consumer
	name     string
}

func (t *subTopic) Subscribe(handler bps.Handler, options ...bps.SubOption) (bps.Subscription, error) {
	opts := (&bps.SubOptions{
		StartAt: bps.PositionNewest,
	}).Apply(options)

	var initialOffset int64
	switch opts.StartAt {
	case bps.PositionNewest:
		initialOffset = sarama.OffsetNewest
	case bps.PositionOldest:
		initialOffset = sarama.OffsetOldest
	default:
		return nil, fmt.Errorf("start position %s is not supported by this implementation", opts.StartAt)
	}

	// get partitions before spawning anything to do less cleanup on failure:
	partitions, err := t.consumer.Partitions(t.name)
	if err != nil {
		return nil, fmt.Errorf("get %s partitions: %w", t.name, err)
	}

	// synchronize handler access:
	handler = bps.SafeHandler(handler)

	// init closeable subscription/thread group
	sub := concurrent.NewGroup(context.Background())

	// spawn partition-consuming threads:
	for _, partition := range partitions {
		if err := t.partition(sub, partition, initialOffset, handler, opts); err != nil {
			_ = sub.Close()
			return nil, fmt.Errorf("consume %s/%d partition: %w", t.name, partition, err)
		}
	}

	return sub, nil
}

func (t *subTopic) partition(sub *concurrent.Group, partition int32, initialOffset int64, handler bps.Handler, opts *bps.SubOptions) error {
	pc, err := t.consumer.ConsumePartition(t.name, partition, initialOffset)
	if err != nil {
		return err
	}

	// close partition consumer when group is finished
	sub.Go(func() {
		defer pc.Close()
		<-sub.Done()
	})

	// subscribe to errors
	sub.Go(func() {
		for err := range pc.Errors() {
			opts.ErrorHandler(fmt.Errorf("consume %s/%d partition: %w", t.name, partition, err))
		}
	})

	// subscribe to messages
	sub.Go(func() {
		for msg := range pc.Messages() {
			handler.Handle(bps.RawSubMessage(msg.Value))
		}
	})

	return nil
}
