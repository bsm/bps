// Package kafka provides a Kafka abstraction through github.com/Shopify/sarama.
//
// Both bps.NewPublisher (`kafka` + `kafka+sync` schemes) and bps.NewConsumer (`kafka` scheme)
// support the following query parameters:
//
//   client.id
//      A user-provided string sent with every request to the brokers for logging debugging, and auditing
//      purposes.
//   kafka.version
//      The version of the kafka server.
//   channel.buffer.size
//      The number of events to buffer in internal and external channels. This permits the producer to
//      continue processing some messages in the background while user code is working, greatly improving
//      throughput (default 256).
//
// bps.NewPublisher supports `kafka` + `kafka+sync` schemes and the following query parameters:
//
//   acks
//      The number of acks required before considering a request complete. When acks=0, the producer will
//      not wait for any acknowledgment. When acks=1 the leader will write the record to its local log but
//      will respond without awaiting full acknowledgement from all followers. When acks=all the leader will
//      wait for the full set of in-sync replicas to acknowledge the record.
//   message.max.bytes
//      The maximum permitted size of a message (default 1,000,000). Should be
//      set equal to or smaller than the broker's `message.max.bytes`.
//   compression.type
//      The compression type for all data generated by the producer. Valid values are: none, gzip, snappy,
//      lz4 (default none).
//   partitioner
//      The partitioner used to partition messages (defaults to hashing the message ID). Valid values are:
//      hash, random, roundrobin,
//   timeout
//      The maximum duration the broker will wait the receipt of the number of acks (default 10s).
//      This is only relevant when acks=all or a number > 1.
//   flush.bytes
//      he best-effort number of bytes needed to trigger a flush.
//   flush.messages
//      The best-effort number of messages needed to trigger a flush.
//   flush.frequency
//      The best-effort frequency of flushes.
//   retry.max
//      The total number of times to retry sending a message (default 3).
//   retry.backoff
//      How long to wait for the cluster to settle between retries (default 100ms).
package kafka

import (
	"context"
	"errors"
	"fmt"
	"net/url"

	"github.com/Shopify/sarama"
	"github.com/bsm/bps"
	"golang.org/x/sync/errgroup"
)

func init() {
	bps.RegisterPublisher("kafka", func(ctx context.Context, u *url.URL) (bps.Publisher, error) {
		config := parseProducerQuery(u.Query())
		config.Producer.Return.Errors = false
		return NewPublisher(parseAddrs(u), config)
	})

	bps.RegisterPublisher("kafka+sync", func(ctx context.Context, u *url.URL) (bps.Publisher, error) {
		config := parseProducerQuery(u.Query())
		config.Producer.Return.Successes = true
		return NewSyncPublisher(parseAddrs(u), config)
	})

	bps.RegisterSubscriber("kafka", func(ctx context.Context, u *url.URL) (bps.Subscriber, error) {
		config := parseSubscriberQuery(u.Query())
		config.Consumer.Return.Errors = false
		return NewSubscriber(parseAddrs(u), config)
	})
}

// --------------------------------------------------------------------

// Publisher wraps a kafka producer and implements the bps.Publisher interface.
type Publisher struct {
	producer sarama.AsyncProducer
}

// NewPublisher inits a new async publisher.
func NewPublisher(addrs []string, config *sarama.Config) (*Publisher, error) {
	producer, err := sarama.NewAsyncProducer(addrs, config)
	if err != nil {
		return nil, err
	}
	return &Publisher{producer: producer}, nil
}

// Topic implements the bps.Publisher interface.
func (p *Publisher) Topic(name string) bps.Topic {
	return &topicAsync{name: name, producer: p.producer}
}

// Close implements the bps.Publisher interface.
func (p *Publisher) Close() error {
	return p.producer.Close()
}

// Producer exposes the native producer. Use at your own risk!
func (p *Publisher) Producer() sarama.AsyncProducer {
	return p.producer
}

type topicAsync struct {
	name     string
	producer sarama.AsyncProducer
}

// Publish implements the bps.Topic interface.
func (t *topicAsync) Publish(_ context.Context, msg *bps.PubMessage) error {
	t.producer.Input() <- convertMessage(t.name, msg)
	return nil
}

// PublishBatch implements the bps.Topic interface.
func (t *topicAsync) PublishBatch(ctx context.Context, batch []*bps.PubMessage) error {
	for _, msg := range batch {
		if err := t.Publish(ctx, msg); err != nil {
			return err
		}
	}
	return nil
}

// --------------------------------------------------------------------

// SyncPublisher wraps a synchronous kafka producer and implements the bps.Publisher interface.
type SyncPublisher struct {
	producer sarama.SyncProducer
}

// NewSyncPublisher inits a new async publisher.
func NewSyncPublisher(addrs []string, config *sarama.Config) (*SyncPublisher, error) {
	producer, err := sarama.NewSyncProducer(addrs, config)
	if err != nil {
		return nil, err
	}
	return &SyncPublisher{producer: producer}, nil
}

// Topic implements the bps.Publisher interface.
func (p *SyncPublisher) Topic(name string) bps.Topic {
	return &topicSync{name: name, producer: p.producer}
}

// Close implements the bps.Publisher interface.
func (p *SyncPublisher) Close() error {
	return p.producer.Close()
}

// Producer exposes the native producer. Use at your own risk!
func (p *SyncPublisher) Producer() sarama.SyncProducer {
	return p.producer
}

type topicSync struct {
	name     string
	producer sarama.SyncProducer
}

// Publish implements the bps.Topic interface.
func (t *topicSync) Publish(_ context.Context, msg *bps.PubMessage) error {
	_, _, err := t.producer.SendMessage(convertMessage(t.name, msg))
	return err
}

// PublishBatch implements the bps.Topic interface.
func (t *topicSync) PublishBatch(ctx context.Context, batch []*bps.PubMessage) error {
	if len(batch) == 0 {
		return nil
	}

	pmsgs := make([]*sarama.ProducerMessage, len(batch))
	for i, msg := range batch {
		pmsgs[i] = convertMessage(t.name, msg)
	}
	return t.producer.SendMessages(pmsgs)
}

// --------------------------------------------------------------------

// Subscriber wraps a kafka consumer and implements the bps.Subscriber interface.
// It starts consuming from the newest offset (so from message, received after connecting to kafka).
type Subscriber struct {
	consumer sarama.Consumer
}

// NewSubscriber inits a new subscriber.
func NewSubscriber(addrs []string, config *sarama.Config) (*Subscriber, error) {
	// TODO: do this on bps.Register:
	// config.Consumer.Return.Errors = false // TODO: it would be actually nice to log these somehow
	consumer, err := sarama.NewConsumer(addrs, config)
	if err != nil {
		return nil, err
	}
	return &Subscriber{consumer: consumer}, nil
}

// Subscribe implements the bps.Subscriber interface.
func (s *Subscriber) Subscribe(ctx context.Context, topic string, handler bps.Handler) error {
	parts, err := s.consumer.Partitions(topic)
	if err != nil {
		return fmt.Errorf("get %s partitions: %w", topic, err)
	}

	group, ctx := errgroup.WithContext(ctx)
	for i := range parts {
		part := parts[i]
		group.Go(func() error {
			csm, err := s.consumer.ConsumePartition(topic, part, sarama.OffsetNewest)
			if err != nil {
				return fmt.Errorf("consume %s/%d partition: %w", topic, part, err)
			}
			defer csm.Close()

			println(
				"consuming",
				"topic", topic,
				"partition", part,
				"HighWaterMarkOffset", csm.HighWaterMarkOffset(),
			)

			for {
				select {
				case <-ctx.Done():
					println("DONE")
					return ctx.Err()

				case msg, more := <-csm.Messages():
					println("MSG")
					if !more {
						return nil
					}
					// TODO: should we bother with locking/channeling messages for handler?
					//       Or just demand bps.Handler to be thread safe?
					if err := handler.Handle(bps.RawSubMessage(msg.Value)); errors.Is(err, bps.Done) {
						return nil
					} else if err != nil {
						return err
					}

					// case err := <-csm.Errors():
					// 	fmt.Printf("ERR %#v\n", err)
				}
			}
		})
	}
	return group.Wait()
}

// Close implements the bps.Subscriber interface.
func (s *Subscriber) Close() error {
	return s.consumer.Close()
}
